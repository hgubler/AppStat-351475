---
title: "Project-1"
author: "Hannes Gubler"
date: "2023-03-03"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this report, we work with data that contains diameter bins of snowflakes described by startpoint and endpoint. More precisely, it describes the total number of particles measured and the percentage of particles belonging to each diameter bin. The data comes from the Laboratory of Cryospheric sciences at EPFL.
Expert knowledge suggests us that a mixture of two log-normal distributions (bi-log-normal distribution) is a good model for the data.
The outline of the remainder of this report is as follows. In Section 2, we visually verify if the expert knowledge of a bi-log-normal distribution is a valid assumption. After that, in Section 3, we will use the EM algorithm and an optimization algorithm to estimate the five parameters that describe distribution. Finally we will use parametric bootstrap to test whether the snowflake diameters come from a bi-log-normal distribution.

# Descriptive Data Exploration

Since we already have access to the expert knowledge telling us that the bi-log-normal distribution is a good model for the data, our main goal in this section is to check whether this is a viable assumption. It would be easy to just plot a histogram of the data, but since we only have the data in bins and not exactly, it will be slightly more difficult. So before plotting the histogram, we will jitter the data using a uniform distribution inside each bin. Also, we will set the endpoint of the last bin to 3.25 to make the size of the last bin more comparable to the rest (The last bin doesn't contain any datapoint so this won't affect our future calculations too much).

```{r histogram, echo=FALSE, fig.align="center", fig.cap="Histogram for the snowflake diameters after jittering the data using a uniform distribution in each bin."}
library(ggplot2)
data = read.csv(file = "1_snow_particles.csv")
num_inbin = rep(0, length(data$X)) # store the number of datapoints in each bin here
num_part = data$particles.detected[1] # number of total datapoints
for (i in 1:length(data$X)) {
  num_inbin[i] = round(data$retained....[i] / 100 * num_part) #rounded number of data in the i-th bin, need to jitter the data in the next step
}
# jittering the data
set.seed(100) # set a seed before jitter the data using a uniform distribution
n = sum(num_inbin) #cnumber of data points
counter = 0
jittered_data = rep(0, n) 
data$endpoint[52] = 3.25 # s.t. all the bins are of comparable size
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    jittered_data[j + counter] = data$startpoint[i] + 
      runif(1, min = 0, max = interval_length)
  }
  counter = counter + num_inbin[i]
}
ggplot(data.frame(jittered_data), aes(x = jittered_data)) + 
  geom_histogram(color="black", binwidth = 0.05) + 
  labs(title = "Histogram of Jittered Data", x = "Jittered Diameters")
#hist(jittered_data, breaks=1000)
```

We can see that the assumption of a bi-log-normal distribution seems valid as we have two bumps that are slightly right scewed, as we would expect from a mixture of two log-normal distributions.

# Parameter estimation

In this section, we will estimate the parameters for the jittered data assuming it follows a bi-log-normal distribution. We will proceed as follows: First, using the EM algorithm, we estimate the parameters $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$ for the jittered data. After that, we optimize the log likelihood of the binned data (not the jittered data as with the EM algorithm) starting with the values given by the EM algorithm. All of these steps will of course be done assuming our data follows the bi-log-normal distribution.

## EM algorithm

To estimate the parameters using the EM algorithm, we have to set a starting value for each parameter $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$. Hence it is useful to look back on our histogram (Figure \@ref(fig:histogram)). Since the expectation and variance of a log-normal distribution with parameters $\mu, \sigma$ are given by $\exp(\mu + \frac{\sigma^2}{2})$ and $(\exp(\sigma^2)-1)\exp(2\mu+\sigma^2)$, we set our starting values to $\mu_1 = -3, \mu_2 = -0.5, \sigma_1 = 0.3, \sigma_2 = 0.4$ and $\tau = 0.7$. We will use the results of the EM algorithm in the next section. The estimates of the EM algorithm give us another opportunity to check the assumption of a bi-log-normal distribution: We can create a plot with the kernel density estimator (Gaussian kernel) and the density of a bi-log-normal distribution parametrized by the results of the EM algorithm.
```{r EM algorithm, echo=FALSE, fig.align='center', fig.cap="Kernel density estimate (gaussian kernel) of the jittered data  together with the density obtained by the bi-log-normal model with the parameters obtained by the EM algorithm."}
# function to sample rom a bi-log-normal distribution
rbilnorm <- function(N, mu1, mu2, sigma1, sigma2, tau){
  ind <- I(runif(N) > tau)
  X <- rep(0,N)
  X[ind] <- rlnorm(sum(ind), mu1, sigma1)
  X[!ind] <- rlnorm(sum(!ind), mu2, sigma2)
  return(X)
}

# pdf for the bi-log-mormal distribution
dbilnorm <- function(x, mu1, mu2, sigma1, sigma2, tau){
  y <- (1-tau)*dlnorm(x, mu1, sigma1) + tau * dlnorm(x, mu2, sigma2)
  return(y)
}

# EM algorithm for the bi-log-normal model
EM = function(x, mu1, mu2, sigma1, sigma2, tau) {
  n = length(x)
  lold = 0
  lnew = 10
  while (abs(lold-lnew) > 10^-2) {
    #calculate log likelihood for convergence criteria
    lold = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
    p = dlnorm(x, mean = mu2, sd = sigma2) * tau / dbilnorm(x, mu1, mu2, sigma1, sigma2, tau)
    #update the parameters according to ML estimates
    tau = 1/n * sum(p)
    mu1 = sum(log(x) * (1 - p)) / sum(1 - p)
    mu2 = sum(log(x) * p) / sum(p)
    sigma1 = sqrt(sum((1 - p) * ((log(x) - mu1)^2)) / sum(1 - p))
    sigma2 = sqrt(sum(p * ((log(x) - mu2)^2)) / sum(p))
    #calculate log likelihood for convergence criteria
    lnew = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
  }
  return(c(mu1, mu2, sigma1, sigma2, tau))
}

# set the starting values by looking at the histogram of the jittered data
mu1 <- -3
mu2 <- -0.5
sigma1 <- 0.3
sigma2 <- 0.4
tau <- 0.7
estim = EM(jittered_data, mu1, mu2, sigma1, sigma2, tau)
#plot the kernel density estimator and bi-log-normal density
grid = seq(from = 0, to = 3.25, by = 0.01)
pdf_est = dbilnorm(grid, estim[1], estim[2], estim[3], estim[4], estim[5])
ggplot() + geom_line(aes(x = grid, y = pdf_est, color = "Bi-Log-Normal Density")) + geom_density(aes(x = jittered_data, color = "Kernel Density Estimate"), kernel = "gaussian") + 
  labs(title = "Bi-Log-Normal Density and Kernel Density Estimate", x = "x", y = "y") + 
  scale_color_manual(name = "",
                     breaks=c('Kernel Density Estimate', 'Bi-Log-Normal Density'),
                     values=c('Kernel Density Estimate'='red', 'Bi-Log-Normal Density'='blue'))
```

We can see two very similar curves, supporting our assumption of the bi-log-normal distribution. Also this plot is useful to see that the EM algorithm worked and converged to a somewhat useful solution.

## Optimization

Since we performed the EM algorithm on the jittered data, we didn't we maximize the true log-likelihood of the binned data. But the results should be close to the maximum of the binned log likelihood, hence we can use them as starting values to perform an optimization on the binned log likelihood. We use the Nelder-Mead optimization algorithm.
```{r optimization, echo=FALSE}
# function calculating the cdf of a bilognornal distribution<
cdfbilognorm = function(x, mu1, mu2, sigma1, sigma2, tau) {
  y <- (1 - tau) * plnorm(x, mu1, sigma1) + tau * plnorm(x, mu2, sigma2)
  return(y)
}

# function calculating the negative log likelihood of the binned data (which we want to minimize)
negloglikelihood = function(par) {
  logli = 0 # start at 0 and sum op at each datapoint
  mu1 = par[1]
  mu2 = par[2]
  sigma1 = par[3]
  sigma2 = par[4]
  tau = par[5]
  for (i in 1:length(data$X)) {
    logli = logli + (num_inbin[i]) * log((cdfbilognorm(data$endpoint[i], mu1, mu2, sigma1, sigma2, tau) - 
                       cdfbilognorm(data$startpoint[i], mu1, mu2, sigma1, sigma2, tau)))
  }
  return(-logli) #return the negative log likelihood
}
out = optim(estim, negloglikelihood, method = "Nelder-Mead")
estim_opt = as.numeric(unlist(out[1]))
mu1_opt = estim_opt[1]
mu2_opt = estim_opt[2]
sigma1_opt = estim_opt[3]
sigma2_opt = estim_opt[4]
tau_opt = estim_opt[5]
```

We know have the optimized parameters for our parametric model of a bi-log-normal distribution. But so far we have only seen visual evidence that the data follows a bi-log-normal distribution. The next section will be dedicated to statistically test this assumption, using parametric bootstrap.

# Parametric Bootstrap

In this section we test whether our data supports the hypothesis if its distribution, say $F$, belongs to the parametric family of bi-log-normal distributions, say $\mathcal{F}$. So we formulate $H_0$ and $H_1$ as
$$
H_0: F \in \mathcal{F},\qquad H_1: F \notin \mathcal{F}.
$$
To perform this test, we proceed as follows: We create XX bootstrap datasets containing XX datapoints by sampling from a bi-log-normal distribution using the optimized parameters from before. After that, for each bootstrap dataset, we perform the following steps.

1. Bin the data using the bins of the original dataset.
2. Jitter the data in the same way as we did with the original dataset.
3. Estimate the parameters of the bi-log-normal model using the EM algorithm.
4. Optimize the true log likelihood of the binned, bootstrapped data.

After that we can estimate the p value for testing $H_0$ vs $H_1$. This is done by comparing on each bootstrapped dataset the empirical CDF with the parametrized CDF using the parameters obtained by EM and Optimization. More precisely we calculate 
$$
T_b^\star = \sup_x \Big| \widehat{F}_{N,b}^\star(x) - F_{\widehat{\lambda}_b^\star}(x) \Big|,
$$
where $\widehat{F}_{N,b}^\star$ is the empirial CDF of of the b-th bootstrapped dataset and $F_{\widehat{\lambda}_b^\star}$ is the CDF parametrized by the optimized parameters ($\lambda_b^\star$) of the b-th bootstrapped parameters. Furthermore we calculate the Kolmogorov-Smirnov statistic of the empirical CDF of our original data and the CDF parametrized by the optimized parameters of the original data, that is 
$$
T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|.
$$
To calculate the empirical CDF of binned data, we distribute the datapoints of each bin on an equidistant grid from the startpoint to the endpoint of this bin.
Now our estimate for the p-value is given by
$$
\widehat{\text{p-val}} = \frac{1}{B+1}\left( 1 + \sum_{b=1}^B \mathbb{I}_{[T_b^\star \geq T]} \right),
$$
where $B =$ XX is the number of bootstrapped datasets. Performing these calculations we will get a p value of XX.
```{r parametric bootstrap, echo=FALSE}
n_bootstrap = 100 # number of bootstrap samples
ndata_boot = 1000 # number of data in bootstrap sample
bootstrap_data = matrix(nrow = ndata_boot, ncol = n_bootstrap) 
for (i in 1:n_bootstrap) {
  bootstrap_data[, i] = rbilnorm(ndata_boot, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
}
#bin the bootstrap data
binned_bootstrap_data = matrix(nrow = length(data$X), ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  for (j in 1:length(data$X)) {
    counter = 0
    for (k in 1:ndata_boot) {
      if (data$startpoint[j] <= bootstrap_data[k, i] && data$endpoint[j] > bootstrap_data[k, i]) {
        counter = counter + 1
      }
    }
    binned_bootstrap_data[j, i] = counter # number of data points in j-th bin
  }
}

# jitter the binned bootstrap samples
jittered_boot_data = matrix(nrow = ndata_boot, ncol = n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      jittered_boot_data[j + counter, k] = data$startpoint[i] + 
        runif(1, min = 0, max = interval_length)
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
}

#estimate the parameters for each bootstrap sample
estim_boot = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  estim_boot[, i] = EM(jittered_boot_data[, i], mu1, mu2, sigma1, sigma2, tau)
}
estim_boot_opt = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  out = optim(estim_boot[, i], negloglikelihood, method="Nelder-Mead")
  estim_boot_opt[, i] = as.numeric(unlist(out[1]))
}
 
#Calculate the kolmogorov smirnoff statisitc
#create data to use for ecdf for binned data
counter = 0
ecdf_data = rep(0, n)
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    if (num_inbin[i] == 0) {
      next
    }
    ecdf_data[j + counter] = data$startpoint[i] + interval_length / j
  }
  counter = counter + num_inbin[i]
}
ecdf_binned = ecdf(ecdf_data)
grid = seq(from = 0, to = 3.25, by = 0.01)
kol_smir = 0 
for (i in 1:length(grid)) {
  a = abs(ecdf_binned(grid[i]) - 
            cdfbilognorm(grid[i], mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt))
  if (a > kol_smir) {
    kol_smir = a
  }
}

#calculate T_b^*
T_b = rep(0, n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  ecdf_boot_data = rep(0, ndata_boot)
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      ecdf_boot_data[j + counter] = data$startpoint[i] + interval_length / j
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
  ecdf_boot = ecdf(ecdf_boot_data)
  for (j in 1:length(grid)) {
    a = abs(ecdf_boot(grid[j]) - 
              cdfbilognorm(grid[j], estim_boot_opt[1, k], estim_boot_opt[2, k], 
                           estim_boot_opt[3, k], estim_boot_opt[4, k], estim_boot_opt[5, k]))
    if (a > T_b[k]) {
      T_b[k] = a
    }
  }
}

#calculate the estimated p value
a = 0
for (k in 1:n_bootstrap) {
  if (T_b[k] > kol_smir) {
    a = a + 1
  }
}
p = 1 / (n_bootstrap + 1) * (a + 1)
print(p)
```
'r toString(p)'

