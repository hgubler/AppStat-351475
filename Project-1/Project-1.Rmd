---
title: "Project-1"
author: "Hannes Gubler"
date: "2023-02-20"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this report, we work with data that contains diameter bins of snowflakes described by startpoint and endpoint. More precisely, it describes the total number of particles measured and the fraction of particles belonging to each diameter bin.
Expert knowledge tells us that a mixture of two log-normal distributions is a good model for the data.
The outline of the remainder of this report is as follows. In Section 2, we verify if the expert knowledge of a mixture of two log-normal distributions is a valid assumption. After that in Section 3 we will use the EM algorithm and an optimization algorithm to estimate the five parameters that describe distribution. Finally, in section 4 we will use parametric bootstrap to test whether the diameters come from a bi-log-normal distribution.

# Descriptive Data Exploration

Since we already have access to the expert knowledge telling us that the bi-log-normal distribution is a good model for the data, our main goal in this section is to check whether this is a viable assumption. It would be easy to just plot a histogram of the data, but since we only have the data in bins and not exactly, it will be slightly more difficult. So before plotting the histogram, we will jitter the data using a uniform distribution inside each bin. Also, we will set the endpoint of the last bin to 3.25 to make the size of the last bin more comparable to the rest (The last bin doesn't contain any datapoint so this won't affect our future calculations too much).

```{r histogram, echo=FALSE, fig.align="center", fig.cap="Histogram for the snowflake diameters after jittering the data using a uniform distribution in each bin."}
library(ggplot2)
data = read.csv(file = "/Users/hannesgubler/Documents/R Repositories/AppStat-351475/Project-1/1_snow_particles.csv")
num_inbin = rep(0, length(data$X)) # store the number of datapoints in each bin here
num_part = data$particles.detected[1] # number of total datapoints
for (i in 1:length(data$X)) {
  num_inbin[i] = round(data$retained....[i] / 100 * num_part) #rounded number of data in the i-th bin, need to jitter the data in the next step
}
# jittering the data
set.seed(1) # set a seed before jitter the data using a uniform distribution
n = sum(num_inbin) #cnumber of data points
counter = 0
jittered_data = rep(0, n) 
data$endpoint[52] = 3.25 # s.t. all the bins are of comparable size
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    jittered_data[j + counter] = data$startpoint[i] + 
      runif(1, min = 0, max = interval_length)
  }
  counter = counter + num_inbin[i]
}
ggplot(data.frame(jittered_data), aes(x = jittered_data)) + 
  geom_histogram(color="black", binwidth = 0.05) + 
  labs(title = "Histogram of Jittered Data", x = "Jittered Diameters")
#hist(jittered_data, breaks=1000)
```

We can see that the assumption of a bi-log-normal distribution seems valid as we have two bumps that are slightly right scewed, as we would expect from a mixture of two log-normal distributions.

# Parameter estimation

In this section, we will estimate the parameters for the jittered data assuming it follows a bi-log-normal distribution. We will proceed as follows: First, using the EM algorithm, we estimate the parameters $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$ for the jittered data. After that, we optimize the log likelihood of the binned data (not the jittered data as with the EM algorithm) starting with the values given by the EM algorithm.

## EM algorithm

To estimate the parameters using the EM algorithm, we have to set a starting value for each parameter $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$. Hence it is useful to look back on our histogram \@ref(fig:histogram). Since the expectation and variance of a lognormal distribution with parameters $\mu_1, \sigma_1$ are given by $\exp(\mu_1 + \frac{\sigma_1^2}{2})$ and $(\exp(\sigma_1^2)-1)\exp(2\mu_1+\sigma_1^2)$, we set our starting values to $\mu_1 = -3, \mu_2 = -0.5, \sigma_1 = 0.3, \sigma_2 = 0.4$ and $\tau = 0.7$. We will use the results of the EM algorithm in the next section.
```{r EM algorithm, echo=FALSE}
# function to sample from a bi-log-normal distribution
rbilnorm <- function(N, mu1, mu2, sigma1, sigma2, tau){
  ind <- I(runif(N) > tau)
  X <- rep(0,N)
  X[ind] <- rlnorm(sum(ind), mu1, sigma1)
  X[!ind] <- rlnorm(sum(!ind), mu2, sigma2)
  return(X)
}

# pdf for the bi-log-mormal distribution
dbilnorm <- function(x, mu1, mu2, sigma1, sigma2, tau){
  y <- (1-tau)*dlnorm(x, mu1, sigma1) + tau * dlnorm(x, mu2, sigma2)
  return(y)
}

# EM algorithm for the bi-log-normal model
EM = function(x, mu1, mu2, sigma1, sigma2, tau) {
  n = length(x)
  lold = 0
  lnew = 10
  while (abs(lold-lnew) > 10^-2) {
    #calculate log likelihood for convergence criteria
    lold = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
    p = dlnorm(x, mean = mu2, sd = sigma2) * tau / dbilnorm(x, mu1, mu2, sigma1, sigma2, tau)
    #update the parameters according to ML estimates
    tau = 1/n * sum(p)
    mu1 = sum(log(x) * (1 - p)) / sum(1 - p)
    mu2 = sum(log(x) * p) / sum(p)
    sigma1 = sqrt(sum((1 - p) * ((log(x) - mu1)^2)) / sum(1 - p))
    sigma2 = sqrt(sum(p * ((log(x) - mu2)^2)) / sum(p))
    #calculate log likelihood for convergence criteria
    lnew = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
  }
  return(c(mu1, mu2, sigma1, sigma2, tau))
}

# set the starting values by looking at the histogram of the jittered data
mu1 <- -3
mu2 <- -0.5
sigma1 <- 0.3
sigma2 <- 0.4
tau <- 0.7
estim = EM(jittered_data, mu1, mu2, sigma1, sigma2, tau)
```

## Optimization

Since we performed the EM algorithm on the jittered data, we didn't we maximize the true log-likelihood of the binned data. But we should be close to the maximum of the binned log likelihood, hence we can use them as starting values to perform an optimization on the binned log likelihood. We use the BFGS optimization algorithm. Once we have the optimized parameters for our bi-log-normal model, we plot the density of this model together with the kernel density estimator of our jittered data, using a gaussian kernel.
```{r optimization, echo=FALSE, fig.align='center', fig.cap="Kernel density estimate of the jittered data together with the density obtained by the bi-log-normal model with our optimized parameters"}
# function calculating the cdf of a bilognornal distribution<
cdfbilognorm = function(x, mu1, mu2, sigma1, sigma2, tau) {
  y <- (1 - tau) * plnorm(x, mu1, sigma1) + tau * plnorm(x, mu2, sigma2)
  return(y)
}

# function calculating the negative log likelihood of the binned data (which we want to minimize)
negloglikelihood = function(par) {
  logli = 0 # start at 0 and sum op at each datapoint
  mu1 = par[1]
  mu2 = par[2]
  sigma1 = par[3]
  sigma2 = par[4]
  tau = par[5]
  for (i in 1:length(data$X)) {
    logli = logli + (num_inbin[i]) * log((cdfbilognorm(data$endpoint[i], mu1, mu2, sigma1, sigma2, tau) - 
                       cdfbilognorm(data$startpoint[i], mu1, mu2, sigma1, sigma2, tau)))
  }
  return(-logli) #return the negative log likelihood
}
out = optim(estim, negloglikelihood, method = "Nelder-Mead")
estim_opt = as.numeric(unlist(out[1]))
mu1_opt = estim_opt[1]
mu2_opt = estim_opt[2]
sigma1_opt = estim_opt[3]
sigma2_opt = estim_opt[4]
tau_opt = estim_opt[5]
grid = seq(from = 0, to = 3.25, by = 0.01)
pdf_est = dbilnorm(grid, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
jittered_pdf = density(x = jittered_data, kernel = "gaussian")
ggplot() + geom_line(aes(x = grid, y = pdf_est, color = "Optimized Density")) + geom_density(aes(x = jittered_data, color = "Kernel Density"), kernel = "gaussian") + 
  labs(x = "x", y = "y") + 
  scale_color_manual(breaks=c('Kernel Density', 'Optimized Density'),
                     values=c('Kernel Density'='red', 'Optimized Density'='blue'))

```

We see that the kernel density estimate and the bi-log-normal density using the optimized parameters are similar, giving us a visual suggestion that the assumption of a bi-log-normal distribution seems reasonable. The next section will be dedicated to test this assumption, using parametric bootstrap.

# Parametric Bootstrap

In this section we test whether our data supports the hypothesis if its distribution, say $F$, belongs to the parametric family of bi-log-normal distributions, say $\mathcal{F}$. So we formulate $H_0$ and $H_1$ as
$$
H_0: F \in \mathcal{F},\qquad H_1: F \notin \mathcal{F}.
$$
To perform this test, we proceed as follows: We create XX bootstrap datasets containing XX datapoints by sampling from a bi-log-normal distribution using the optimized parameters from before. After that, for each bootstrap dataset, we perform the following steps.

1. Bin the data using the bins of the original dataset.
2. Jitter the data in the same way as we did with the original dataset.
3. Estimate the parameters of the bi-log-normal model using the EM algorithm.
4. Optimize the true log likelihood of the binned, bootstrapped data.

After that we can estimate the p value for testing $H_0$ vs $H_1$. This is done by comparing the kolmogorov statistic to 
```{r parametric bootstrap, echo=FALSE}
n_bootstrap = 100 # number of bootstrap samples
ndata_boot = 1000 # number of data in bootstrap sample
bootstrap_data = matrix(nrow = ndata_boot, ncol = n_bootstrap) 
for (i in 1:n_bootstrap) {
  bootstrap_data[, i] = rbilnorm(ndata_boot, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
}
#bin the bootstrap data
binned_bootstrap_data = matrix(nrow = length(data$X), ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  for (j in 1:length(data$X)) {
    counter = 0
    for (k in 1:ndata_boot) {
      if (data$startpoint[j] <= bootstrap_data[k, i] && data$endpoint[j] > bootstrap_data[k, i]) {
        counter = counter + 1
      }
    }
    binned_bootstrap_data[j, i] = counter # number of data points in j-th bin
  }
}

# jitter the binned bootstrap samples
jittered_boot_data = matrix(nrow = ndata_boot, ncol = n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      jittered_boot_data[j + counter, k] = data$startpoint[i] + 
        runif(1, min = 0, max = interval_length)
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
}

#estimate the parameters for each bootstrap sample
estim_boot = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  estim_boot[, i] = EM(jittered_boot_data[, i], mu1, mu2, sigma1, sigma2, tau)
}
estim_boot_opt = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  out = optim(estim_boot[, i], negloglikelihood, method="Nelder-Mead")
  estim_boot_opt[, i] = as.numeric(unlist(out[1]))
}
 
#Calculate the kolmogorov smirnoff statisitc
#create data to use for ecdf for binned data
counter = 0
ecdf_data = rep(0, n)
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    if (num_inbin[i] == 0) {
      next
    }
    ecdf_data[j + counter] = data$startpoint[i] + interval_length / j
  }
  counter = counter + num_inbin[i]
}
ecdf_binned = ecdf(ecdf_data)
grid = seq(from = 0, to = 3.25, by = 0.01)
kol_smir = 0 
for (i in 1:length(grid)) {
  a = abs(ecdf_binned(grid[i]) - 
            cdfbilognorm(grid[i], mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt))
  if (a > kol_smir) {
    kol_smir = a
  }
}

#calculate T_b^*
T_b = rep(0, n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  ecdf_boot_data = rep(0, ndata_boot)
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      ecdf_boot_data[j + counter] = data$startpoint[i] + interval_length / j
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
  ecdf_boot = ecdf(ecdf_boot_data)
  for (j in 1:length(grid)) {
    a = abs(ecdf_boot(grid[j]) - 
              cdfbilognorm(grid[j], estim_boot_opt[1, k], estim_boot_opt[2, k], 
                           estim_boot_opt[3, k], estim_boot_opt[4, k], estim_boot_opt[5, k]))
    if (a > T_b[k]) {
      T_b[k] = a
    }
  }
}

#calculate the estimated p value
a = 0
for (k in 1:n_bootstrap) {
  if (T_b[k] > kol_smir) {
    a = a + 1
  }
}
p = 1 / (n_bootstrap + 1) * (a + 1)
print(p)
```
'r toString(p)'

