---
title: "Project-1"
author: "Hannes Gubler"
date: "2023-03-03"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this report, we work with binned data where each bin refers to a diameter range of snowflakes .More precisely, it describes the total number of snowflakes measured and the percentage of snowflakes belonging to each diameter bin. The data comes from the Laboratory of Cryospheric sciences at EPFL.
Expert knowledge suggests us that a mixture of two log-normal distributions (bi-log-normal distribution) is a good model for the data.
The outline of the remainder of this report is as follows: In Section 2, we visually verify if the expert knowledge of a bi-log-normal distribution is a valid assumption. After that, in Section 3, we will use the EM algorithm and an optimization algorithm to estimate the five parameters that describe distribution. Finally we will use parametric bootstrap to test whether the snowflake diameters come from a bi-log-normal distribution.

# Descriptive Data Exploration

Since we already have access to the expert knowledge telling us that the bi-log-normal distribution is a good model for the data, our main goal in this section is to check whether this is a viable assumption. It would be easy to just plot a histogram of the data, but since we only have the data in bins of different sizes, it will be slightly more difficult. So before plotting the histogram, we will jitter the data using a uniform distribution inside each bin. Also, we will set the endpoint of the last bin to 3.25 to make the size of the last bin more comparable to the rest (the last bin doesn't contain any datapoint so this won't affect our future calculations).

```{r histogram, echo=FALSE, fig.align="center", fig.cap="Histogram for the snowflake diameters after jittering the data using a uniform distribution in each bin."}
library(ggplot2)
data = read.csv(file = "1_snow_particles.csv")
num_inbin = rep(0, length(data$X)) # store the number of datapoints in each bin here
num_part = data$particles.detected[1] # number of total datapoints
for (i in 1:length(data$X)) {
  num_inbin[i] = round(data$retained....[i] / 100 * num_part) #rounded number of data in the i-th bin, need to jitter the data in the next step
}
# jittering the data
set.seed(100) # set a seed before jitter the data using a uniform distribution
n = sum(num_inbin) #cnumber of data points
counter = 0
jittered_data = rep(0, n) 
data$endpoint[52] = 3.25 # s.t. all the bins are of comparable size
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    jittered_data[j + counter] = data$startpoint[i] + 
      runif(1, min = 0, max = interval_length)
  }
  counter = counter + num_inbin[i]
}
ggplot(data.frame(jittered_data), aes(x = jittered_data)) + 
  geom_histogram(color="black", binwidth = 0.05) + 
  labs(title = "Histogram of Jittered Data", x = "Jittered Diameters")
#hist(jittered_data, breaks=1000)
```

We can see that the assumption of a bi-log-normal distribution seems valid as we have two bumps that are slightly right scewed, as we would expect from a mixture of two log-normal distributions.

# Parameter estimation

In this section, we will estimate the parameters for the jittered data assuming it follows a bi-log-normal distribution. We will proceed as follows: First, using the EM algorithm, we estimate the parameters $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$ for the jittered data. After that, we optimize the log likelihood of the binned data (not the jittered data we use for the EM algorithm) starting with the values given by the EM algorithm. 

## EM algorithm

To estimate the parameters using the EM algorithm, we have to set a starting value for each parameter $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$. Hence it is useful to look back on our histogram (Figure \@ref(fig:histogram)). Since the expectation and variance of a log-normal distribution with parameters $\mu, \sigma$ are given by $\exp(\mu + \frac{\sigma^2}{2})$ and $(\exp(\sigma^2)-1)\exp(2\mu+\sigma^2)$, we set our starting values to $\mu_1 = -3, \mu_2 = -0.5, \sigma_1 = 0.3, \sigma_2 = 0.4$ and $\tau = 0.7$. We will use the results of the EM algorithm in the next section. For now, the estimates of the EM algorithm give us another opportunity to check the assumption of a bi-log-normal model: We can create a plot with the kernel density estimator (Gaussian kernel) of the jittered data and the density of a bi-log-normal distribution parametrized by the results of the EM algorithm.
```{r EM, echo=FALSE, fig.align='center', fig.cap="Kernel density estimate (Gaussian kernel) of the jittered data  together with the density obtained by the bi-log-normal model with the parameters obtained by the EM algorithm."}
# function to sample rom a bi-log-normal distribution
rbilnorm <- function(N, mu1, mu2, sigma1, sigma2, tau){
  ind <- I(runif(N) > tau)
  X <- rep(0,N)
  X[ind] <- rlnorm(sum(ind), mu1, sigma1)
  X[!ind] <- rlnorm(sum(!ind), mu2, sigma2)
  return(X)
}

# pdf for the bi-log-mormal distribution
dbilnorm <- function(x, mu1, mu2, sigma1, sigma2, tau){
  y <- (1-tau)*dlnorm(x, mu1, sigma1) + tau * dlnorm(x, mu2, sigma2)
  return(y)
}

# EM algorithm for the bi-log-normal model
EM = function(x, mu1, mu2, sigma1, sigma2, tau) {
  n = length(x)
  lold = 0
  lnew = 10
  while (abs(lold-lnew) > 10^-2) {
    #calculate log likelihood for convergence criteria
    lold = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
    p = dlnorm(x, mean = mu2, sd = sigma2) * tau / dbilnorm(x, mu1, mu2, sigma1, sigma2, tau)
    #update the parameters according to ML estimates
    tau = 1/n * sum(p)
    mu1 = sum(log(x) * (1 - p)) / sum(1 - p)
    mu2 = sum(log(x) * p) / sum(p)
    sigma1 = sqrt(sum((1 - p) * ((log(x) - mu1)^2)) / sum(1 - p))
    sigma2 = sqrt(sum(p * ((log(x) - mu2)^2)) / sum(p))
    #calculate log likelihood for convergence criteria
    lnew = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
  }
  return(c(mu1, mu2, sigma1, sigma2, tau))
}

# set the starting values by looking at the histogram of the jittered data
mu1 <- -3
mu2 <- -0.5
sigma1 <- 0.3
sigma2 <- 0.4
tau <- 0.7
estim = EM(jittered_data, mu1, mu2, sigma1, sigma2, tau)
#plot the kernel density estimator and bi-log-normal density
grid = seq(from = 0, to = 3.25, by = 0.01)
pdf_est = dbilnorm(grid, estim[1], estim[2], estim[3], estim[4], estim[5])
ggplot() + geom_line(aes(x = grid, y = pdf_est, color = "Bi-Log-Normal Density")) + geom_density(aes(x = jittered_data, color = "Kernel Density Estimate"), kernel = "gaussian") + 
  labs(title = "Bi-Log-Normal Density and Kernel Density Estimate", x = "x", y = "y") + 
  scale_color_manual(name = "",
                     breaks=c('Kernel Density Estimate', 'Bi-Log-Normal Density'),
                     values=c('Kernel Density Estimate'='red', 'Bi-Log-Normal Density'='blue'))
```

We can see two very similar curves, supporting our assumption of the bi-log-normal model. This plot is also useful to see that the EM algorithm converged to a reasonable solution.

## Optimization

Since we performed the EM algorithm on the jittered data, we didn't we maximize the true log-likelihood of the binned data. The log likelihood of the binned data is given by
$$
f(d\mid\mu_1, \mu_2,\sigma_1, \sigma_2, \tau) \propto \prod_{j=1}^n \left[ \Phi_{\mu_1, \mu_2,\sigma_1, \sigma_2, \tau}(a_{j}) - \Phi_{\mu_1, \mu_2,\sigma_1, \sigma_2, \tau}(a_{j-1}) \right]^{b_j},
$$
where $a_j$ are the bin endpoints, $b_j$ the frequencies observed for each bin and $d$ is the full dataset.
Still, the results from the EM algorithm should be close to the maximum of the log likelihood, hence we can use them as starting values to perform an optimization. We use the Nelder-Mead optimization algorithm.
```{r optimization, echo=FALSE}
# function calculating the cdf of a bilognornal distribution<
cdfbilognorm = function(x, mu1, mu2, sigma1, sigma2, tau) {
  y <- (1 - tau) * plnorm(x, mu1, sigma1) + tau * plnorm(x, mu2, sigma2)
  return(y)
}

# function calculating the negative log likelihood of the binned data (which we want to minimize)
negloglikelihood = function(par) {
  logli = 0 # start at 0 and sum op at each datapoint
  mu1 = par[1]
  mu2 = par[2]
  sigma1 = par[3]
  sigma2 = par[4]
  tau = par[5]
  for (i in 1:length(data$X)) {
    logli = logli + (num_inbin[i]) * log((cdfbilognorm(data$endpoint[i], mu1, mu2, sigma1, sigma2, tau) - 
                       cdfbilognorm(data$startpoint[i], mu1, mu2, sigma1, sigma2, tau)))
  }
  return(-logli) #return the negative log likelihood
}
out = optim(estim, negloglikelihood, method = "Nelder-Mead")
estim_opt = as.numeric(unlist(out[1]))
mu1_opt = estim_opt[1]
mu2_opt = estim_opt[2]
sigma1_opt = estim_opt[3]
sigma2_opt = estim_opt[4]
tau_opt = estim_opt[5]
```

We know have the optimized parameters for our parametric model of a bi-log-normal distribution. But so far we have only seen visual evidence that the data follows a bi-log-normal distribution. The next section will be dedicated to statistically test this assumption, using parametric bootstrap.

# Parametric Bootstrap

In this section we test whether our data supports the hypothesis if its distribution, say $F$, belongs to the parametric family of bi-log-normal distributions, say $\mathcal{F}$. So we formulate $H_0$ and $H_1$ as
$$
H_0: F \in \mathcal{F},\qquad H_1: F \notin \mathcal{F}.
$$
To perform this test, we proceed as follows: We create 100 bootstrap datasets containing 500 datapoints by sampling from a bi-log-normal distribution using the optimized parameters from before. After that, for each bootstrap dataset, we perform the following steps.

1. Bin the data using the bins of the original dataset.
2. Jitter the data in the same way as we did with the original dataset.
3. Estimate the parameters of the bi-log-normal model using the EM algorithm.
4. Optimize the true log likelihood of the binned, bootstrapped data.

After that we can estimate the p value for testing $H_0$ vs $H_1$. This is done by comparing on each bootstrapped dataset the empirical CDF with the parametrized CDF using the parameters obtained by EM and Optimization. More precisely we calculate 
$$
T_b^\star = \sup_x \Big| \widehat{F}_{N,b}^\star(x) - F_{\widehat{\lambda}_b^\star}(x) \Big|,
$$
where $\widehat{F}_{N,b}^\star$ is the empirial CDF of the binned b-th bootstrapped dataset and $F_{\widehat{\lambda}_b^\star}$ is the CDF parametrized by the optimized parameters ($\lambda_b^\star$) using the binned b-th bootstrapped dataset. Furthermore we calculate the Kolmogorov-Smirnov statistic of the empirical CDF of our original binned data and the CDF parametrized by the optimized parameters of the original data, that is 
$$
T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|.
$$
To calculate the empirical CDF of binned data, we put the weight of each observation at the endpoint of the corresponding bin (the CDF gives the probability of a random variable being smaller than a particular value, that's why it seems reasonable to only consider the endpoints of the bins).
Now our estimate for the p-value is given by
$$
\widehat{\text{p-val}} = \frac{1}{B+1}\left( 1 + \sum_{b=1}^B \mathbb{I}_{[T_b^\star \geq T]} \right),
$$
where $B =$ 100 is the number of bootstrapped datasets. 
```{r parametric bootstrap, echo=FALSE}
n_bootstrap = 100 # number of bootstrap samples
ndata_boot = 500 # number of data in bootstrap sample
bootstrap_data = matrix(nrow = ndata_boot, ncol = n_bootstrap) 
for (i in 1:n_bootstrap) {
  bootstrap_data[, i] = rbilnorm(ndata_boot, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
}
#bin the bootstrap data
binned_bootstrap_data = matrix(nrow = length(data$X), ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  for (j in 1:length(data$X)) {
    counter = 0
    for (k in 1:ndata_boot) {
      if (data$startpoint[j] <= bootstrap_data[k, i] && data$endpoint[j] > bootstrap_data[k, i]) {
        counter = counter + 1
      }
    }
    binned_bootstrap_data[j, i] = counter # number of data points in j-th bin
  }
}

# jitter the binned bootstrap samples
jittered_boot_data = matrix(nrow = ndata_boot, ncol = n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      jittered_boot_data[j + counter, k] = data$startpoint[i] + 
        runif(1, min = 0, max = interval_length)
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
}

#estimate the parameters for each bootstrap sample
estim_boot = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  estim_boot[, i] = EM(jittered_boot_data[, i], mu1, mu2, sigma1, sigma2, tau)
}
estim_boot_opt = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  out = optim(estim_boot[, i], negloglikelihood, method="Nelder-Mead")
  estim_boot_opt[, i] = as.numeric(unlist(out[1]))
}
 
#Calculate the kolmogorov smirnoff statisitc
#create data to use for ecdf for binned data
counter = 0
ecdf_data = rep(0, n)
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    if (num_inbin[i] == 0) {
      next
    }
    #ecdf_data[j + counter] = data$startpoint[i] + interval_length / j
    ecdf_data[j + counter] = data$endpoint[i]
  }
  counter = counter + num_inbin[i]
}
ecdf_binned = ecdf(ecdf_data)
grid = seq(from = 0, to = 3.25, by = 0.01)
kol_smir = 0 
for (i in 1:length(grid)) {
  a = abs(ecdf_binned(grid[i]) - 
            cdfbilognorm(grid[i], mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt))
  if (a > kol_smir) {
    kol_smir = a
  }
}

#calculate T_b^*
T_b = rep(0, n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  ecdf_boot_data = rep(0, ndata_boot)
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      #ecdf_boot_data[j + counter] = data$startpoint[i] + interval_length / j
      ecdf_boot_data[j + counter] = data$endpoint[i] 
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
  ecdf_boot = ecdf(ecdf_boot_data)
  for (j in 1:length(grid)) {
    a = abs(ecdf_boot(grid[j]) - 
              cdfbilognorm(grid[j], estim_boot_opt[1, k], estim_boot_opt[2, k], 
                           estim_boot_opt[3, k], estim_boot_opt[4, k], estim_boot_opt[5, k]))
    if (a > T_b[k]) {
      T_b[k] = a
    }
  }
}

#calculate the estimated p value
a = 0
for (k in 1:n_bootstrap) {
  if (T_b[k] > kol_smir) {
    a = a + 1
  }
}
p = 1 / (n_bootstrap + 1) * (a + 1)
#print(p)
```
Performing these calculations we ovserve a p-value of 0.7029703. This is quite a large p-value and hence we will not reject $H_0$, supporting the assumption of a bi-log-normal model for our data. So we can use our optimized parameters to simulate new datapoints for our snowflake experiment. To finish this section, we provide a final plot containing the empirical CDF of our data and the parametrized CDF using our optimized parameters.

```{r CDF, echo=FALSE, fig.align='center', fig.cap="Empirical CDF of the original binned data with a bi-log-normal CDF parametrized by our optimized parameters."}
ecdf_param = cdfbilognorm(grid, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
ggplot() + stat_ecdf(aes(x = ecdf_data, color = "Empirical CDF")) + geom_line(aes(x = grid, y = ecdf_param, color = "Bi-Log-Normal CDF")) + 
  scale_color_manual(name = "",
                     breaks=c('Empirical CDF', 'Bi-Log-Normal CDF'),
                     values=c('Empirical CDF'='red', 'Bi-Log-Normal CDF'='blue')) + 
  labs(x = "x", y = "y", title = "Bi-Log-Normal CDF and Empirical CDF")
```
We see that our bi-log-normal distribution parametrized by our optimized parameters is a very good approximation to the binned data.

# Conclusion

In this report, we were able to find statistical arguments to show that the snowflake diameters data follows a bi-log-normal distribution. In addition to that, using the EM algorithm and optimization of the log likelihood, we were able to find a set of parameters such that our bi-log-normal model describes the data very well. The steps in this report also show that even if we only have access to binned data there are possibilities to find an appropriate statistical model for the data. However this requires some extra steps and we would always prefer non-binned data as it provides more information.