---
title: "Project-1"
author: "Hannes Gubler"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

In this report, we work with binned data where each bin refers to a diameter range of snowflakes. More precisely, it contains the total number of snowflakes measured and the percentage of snowflakes belonging to each diameter bin. The data comes from the Laboratory of Cryospheric sciences at EPFL.
Expert knowledge suggests that a mixture of two log-normal distributions (bi-log-normal distribution) is a good model for the data.

The outline of the remainder of this report is as follows: In Section (\@ref(expl)), we visually verify whether the expert knowledge of a bi-log-normal distribution is a valid assumption. After that, in Section (\@ref(estim)), we will use the EM algorithm and an optimization algorithm to estimate the five parameters that describe the bi-log-normal distribution. Finally, in Section (\@ref(boot)), we will use parametric bootstrap to test whether the snowflake diameters come from a bi-log-normal distribution.

# Descriptive Data Exploration {#expl}

Since we already have access to the expert knowledge telling us that the bi-log-normal distribution is a good model for the data, our main goal in this section is to check whether this is a viable assumption. It would be easy to just plot a histogram of the data, but since we only have the data in bins of different sizes, it will be slightly more difficult. So before plotting the histogram in Figure (\@ref(fig:histogram)), we jitter the data using a uniform distribution inside each bin. Also, we set the endpoint of the last bin to 3.25 to make the size of the last bin more comparable to the other bins (the last bin doesn't contain any datapoint so this will not affect our future calculations).

```{r histogram, echo=FALSE, fig.align="center", fig.cap="Histogram of the snowflake diameters after jittering the data using a uniform distribution in each bin."}
library("ggplot2")
library("knitr")
set.seed(1) # set a seed for reproducibility
data = read.csv(file = "1_snow_particles.csv")
num_inbin = rep(0, length(data$X)) # store the number of datapoints in each bin here
num_part = data$particles.detected[1] # number of total datapoints
for (i in 1:length(data$X)) {
  num_inbin[i] = round(data$retained....[i] / 100 * num_part) #rounded number of data in the i-th bin, need to jitter the data in the next step
}
# jittering the data
n = sum(num_inbin) #cnumber of data points
counter = 0
jittered_data = rep(0, n) 
data$endpoint[52] = 3.25 # s.t. all the bins are of comparable size
for (i in 1:length(num_inbin)) {
  interval_length = data$endpoint[i] - data$startpoint[i]
  for(j in 1:num_inbin[i]) {
    jittered_data[j + counter] = data$startpoint[i] + 
      runif(1, min = 0, max = interval_length)
  }
  counter = counter + num_inbin[i]
}
ggplot(data.frame(jittered_data), aes(x = jittered_data)) + 
  geom_histogram(color="black", binwidth = 0.05) + 
  labs(title = "Histogram of Jittered Data", x = "Jittered Diameters")
#hist(jittered_data, breaks=1000)
```

We can see that the assumption of a bi-log-normal distribution seems valid as we have two bumps that are slightly right scewed, as we would expect from a mixture of two log-normal distributions.

# Parameter estimation {#estim}

In this section, we estimate the parameters for the jittered data assuming it follows a bi-log-normal distribution. We proceed as follows: First, using the EM algorithm, we estimate the parameters $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$ for the jittered data. After that, we optimize the log likelihood of the binned data (not the jittered data we use for the EM algorithm) starting with the values given by the EM algorithm. 

## EM algorithm {#EM}

To estimate the parameters using the EM algorithm, we have to set a starting value for each parameter $\mu_1, \mu_2, \sigma_1, \sigma_2$ and $\tau$. Hence it is useful to look back on our histogram in Figure (\@ref(fig:histogram)). Since the expectation and variance of a log-normal distribution with parameters $\mu, \sigma$ are given by $\exp(\mu + \frac{\sigma^2}{2})$ and $(\exp(\sigma^2)-1)\exp(2\mu+\sigma^2)$, we set our starting values to $\mu_1 = -3, \mu_2 = -0.5, \sigma_1 = 0.3, \sigma_2 = 0.4$ and $\tau = 0.7$. We will use the estimates of the EM algorithm in the next section to optimize the likelihood of the binned data. For now, the estimates of the EM algorithm give us another opportunity to check the assumption of a bi-log-normal model: We can create a plot with the kernel density estimator (Gaussian kernel) of the jittered data and the density of a bi-log-normal distribution parametrized by the estimates of the EM algorithm.
```{r EM, echo=FALSE, fig.align='center', fig.cap="Kernel density estimate (Gaussian kernel) of the jittered data  together with the density of the bi-log-normal model parametrized by the estimates of the EM algorithm applied to the jittered data."}
# function to sample rom a bi-log-normal distribution
rbilnorm <- function(N, mu1, mu2, sigma1, sigma2, tau){
  ind <- I(runif(N) > tau)
  X <- rep(0,N)
  X[ind] <- rlnorm(sum(ind), mu1, sigma1)
  X[!ind] <- rlnorm(sum(!ind), mu2, sigma2)
  return(X)
}

# pdf for the bi-log-mormal distribution
dbilnorm <- function(x, mu1, mu2, sigma1, sigma2, tau){
  y <- (1-tau)*dlnorm(x, mu1, sigma1) + tau * dlnorm(x, mu2, sigma2)
  return(y)
}

# EM algorithm for the bi-log-normal model
EM = function(x, mu1, mu2, sigma1, sigma2, tau) {
  n = length(x)
  lold = 0
  lnew = 10
  while (abs(lold-lnew) > 10^-2) {
    #calculate log likelihood for convergence criteria
    lold = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
    p = dlnorm(x, mean = mu2, sd = sigma2) * tau / dbilnorm(x, mu1, mu2, sigma1, sigma2, tau)
    #update the parameters according to ML estimates
    tau = 1/n * sum(p)
    mu1 = sum(log(x) * (1 - p)) / sum(1 - p)
    mu2 = sum(log(x) * p) / sum(p)
    sigma1 = sqrt(sum((1 - p) * ((log(x) - mu1)^2)) / sum(1 - p))
    sigma2 = sqrt(sum(p * ((log(x) - mu2)^2)) / sum(p))
    #calculate log likelihood for convergence criteria
    lnew = sum(log((1 - tau)*dlnorm(x, mean = mu1, sd = sigma1) + tau * dlnorm(x, mean = mu2, sd = sigma2)))
  }
  return(c(mu1, mu2, sigma1, sigma2, tau))
}

# set the starting values by looking at the histogram of the jittered data
mu1 <- -3
mu2 <- -0.5
sigma1 <- 0.3
sigma2 <- 0.4
tau <- 0.7
estim = EM(jittered_data, mu1, mu2, sigma1, sigma2, tau)
#plot the kernel density estimator and bi-log-normal density
grid = seq(from = 0, to = 3.25, by = 0.01)
pdf_est = dbilnorm(grid, estim[1], estim[2], estim[3], estim[4], estim[5])
ggplot() + 
  geom_line(aes(x = grid, y = pdf_est, color = "Bi-Log-Normal Density")) + 
  geom_density(aes(x = jittered_data, color = "Kernel Density Estimate"),
               kernel = "gaussian") + 
  labs(title = "Bi-Log-Normal Density and Kernel Density Estimate", x = "x", y = "y") + 
  scale_color_manual(name = "",
                     breaks=c('Kernel Density Estimate', 'Bi-Log-Normal Density'),
                     values=c('Kernel Density Estimate'='red',
                              'Bi-Log-Normal Density'='blue'))
```

We can see two very similar curves, supporting our assumption of the bi-log-normal model. This plot is also useful to see that the EM algorithm converged to a reasonable solution.

## Optimization

Since we performed the EM algorithm on the jittered data, we did not maximize the true log-likelihood of the binned data. The log likelihood of the binned data is given by
$$
f(d\mid\mu_1, \mu_2,\sigma_1, \sigma_2, \tau) \propto \prod_{j=1}^n \left[ \Phi_{\mu_1, \mu_2,\sigma_1, \sigma_2, \tau}(a_{j}) - \Phi_{\mu_1, \mu_2,\sigma_1, \sigma_2, \tau}(a_{j-1}) \right]^{b_j},
$$
where $a_j$ are the bin endpoints, $b_j$ the frequencies observed for each bin, $\Phi$ is the CDF of the bi-log-normal distribution and $d$ is the full dataset.

Even tough the estimates from the EM algorithm in Section (\@ref(EM)) are for the jittered data, the results should be close to the MLE of the binned log likelihood, hence we can use them as starting values to perform an optimization on the binned log likelihood. We use the Nelder-Mead optimization algorithm.
```{r optimization, echo=FALSE}
# function calculating the cdf of a bilognornal distribution<
cdfbilognorm = function(x, mu1, mu2, sigma1, sigma2, tau) {
  y <- (1 - tau) * plnorm(x, mu1, sigma1) + tau * plnorm(x, mu2, sigma2)
  return(y)
}

# function calculating the negative log likelihood of the binned data (which we want to minimize)
negloglikelihood = function(par, data, num_inbin) {
  logli = 0 # start at 0 and sum op at each datapoint
  mu1 = par[1]
  mu2 = par[2]
  sigma1 = par[3]
  sigma2 = par[4]
  tau = par[5]
  for (i in 1:length(data$X)) {
    logli = logli + (num_inbin[i]) * log((cdfbilognorm(data$endpoint[i], mu1, mu2, sigma1, sigma2, tau) - 
                       cdfbilognorm(data$startpoint[i], mu1, mu2, sigma1, sigma2, tau)))
  }
  return(-logli) #return the negative log likelihood
}

out = optim(par = estim, fn = negloglikelihood, method = "Nelder-Mead", data = data,
            num_inbin = num_inbin)
estim_opt = as.numeric(unlist(out[1]))
mu1_opt = estim_opt[1]
mu2_opt = estim_opt[2]
sigma1_opt = estim_opt[3]
sigma2_opt = estim_opt[4]
tau_opt = estim_opt[5]
```

The estimates we obtain by optimizing the binned log likelihood are displayed in Table (\@ref(tab:paramtable)).

```{r paramtable}
df_table = data.frame(
  Name = c("$\\mu_1$", "$\\mu_2$", "$\\sigma_1$", "$\\sigma_2$", "$\\tau$"),
  Value = estim_opt
)
kable(df_table, caption = "Parameter estimates obtained by optimizing the binned log likelihood using the results of the EM algorithm on the jittered data as starting point.", digits = 3)
```

The following steps describe how to simulate snoflake diameters using the parameters from Table (\@ref(tab:paramtable)).

* Generate a random sample $x$ from a Bernoulli distribution with parameter $\tau$.
  - If $x = 0$ generate one random sample from a bi-log-normal distribution with parameters $\mu_1, \sigma_1$.
  - If $x = 1$ generate one random sample from a bi-log-normal distribution with parameters $\mu_2, \sigma_2$.

So far we have only seen visual evidence that the data could follow a bi-log-normal distribution. Section (\@ref(boot)) will be dedicated to statistically test this assumption, using parametric bootstrap.

# Parametric Bootstrap {#boot}

In this section we test whether our data supports the hypothesis if its distribution, say $F$, belongs to the parametric family of bi-log-normal distributions, say $\mathcal{F}$. So we formulate $H_0$ and $H_1$ as
$$
H_0: F \in \mathcal{F},\qquad H_1: F \notin \mathcal{F}.
$$
To perform this test, we proceed as follows: We create 100 bootstrap datasets containing 705044 datapoints (same number as original dataset) by sampling from a bi-log-normal distribution using the optimized parameters from before. After that, for each bootstrap dataset, we perform the following steps.

1. Bin the data using the bins of the original dataset.
2. Jitter the data in the same way as we did with the original dataset.
3. Estimate the parameters of the bi-log-normal model using the EM algorithm on the jittered data.
4. Optimize the log likelihood of the binned data.

After that we can estimate the p-value for testing $H_0$ vs $H_1$. This is done by comparing on each bootstrapped dataset the empirical CDF with the parametrized CDF using the parameters obtained by EM and Optimization. More precisely we calculate the Kolmogorov-Smirnov statistic
$$
T_b^\star = \sup_x \Big| \widehat{F}_{N,b}^\star(x) - F_{\widehat{\lambda}_b^\star}(x) \Big|,
$$
where $\widehat{F}_{N,b}^\star$ is the empirical CDF of the binned $b$-th bootstrapped dataset and $F_{\widehat{\lambda}_b^\star}$ is the CDF parametrized by the estimated parameters $\lambda_b^\star$, obtained by optimizing the binned log likelihood of the binned $b$-th bootstrapped dataset. Furthermore we calculate the Kolmogorov-Smirnov statistic of the empirical CDF of our original binned data and the CDF parametrized by the optimized parameters $\widehat{\lambda}$ of the original data, that is 
$$
T = \sup_x \Big| \widehat{F}_N(x) - F_\widehat{\lambda}(x) \Big|.
$$
To calculate the empirical CDF of binned data, we put the weight of each observation at the endpoint of the corresponding bin (the CDF gives the probability of a random variable being smaller than a particular value, that's why it seems reasonable to only consider the endpoints of the bins).
Now our estimate for the p-value is given by
$$
\widehat{\text{p-val}} = \frac{1}{B+1}\left( 1 + \sum_{b=1}^B \mathbb{I}_{[T_b^\star \geq T]} \right),
$$
where $B =$ 100 is the number of bootstrapped datasets. 
```{r parametric bootstrap, echo=FALSE}
n_bootstrap = 100 # number of bootstrap samples
ndata_boot = 500 # number of data in bootstrap sample
bootstrap_data = matrix(nrow = ndata_boot, ncol = n_bootstrap) 
for (i in 1:n_bootstrap) {
  bootstrap_data[, i] = rbilnorm(ndata_boot, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
}
#bin the bootstrap data
binned_bootstrap_data = matrix(nrow = length(data$X), ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  for (j in 1:length(data$X)) {
    counter = 0
    for (k in 1:ndata_boot) {
      if (data$startpoint[j] <= bootstrap_data[k, i] && data$endpoint[j] > bootstrap_data[k, i]) {
        counter = counter + 1
      }
    }
    binned_bootstrap_data[j, i] = counter # number of data points in j-th bin
  }
}

# jitter the binned bootstrap samples
jittered_boot_data = matrix(nrow = ndata_boot, ncol = n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  for (i in 1:length(data$X)) {
    interval_length = data$endpoint[i] - data$startpoint[i]
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      jittered_boot_data[j + counter, k] = data$startpoint[i] + 
        runif(1, min = 0, max = interval_length)
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
}

#estimate the parameters for each bootstrap sample
estim_boot = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  estim_boot[, i] = EM(jittered_boot_data[, i], mu1, mu2, sigma1, sigma2, tau)
}

#use optimizetion as before for the binned bootstrap data
estim_boot_opt = matrix(nrow = 5, ncol = n_bootstrap)
for (i in 1:n_bootstrap) {
  out = optim(par = estim_boot[, i], fn = negloglikelihood, method="Nelder-Mead",
              data = data, num_inbin = binned_bootstrap_data[, i])
  estim_boot_opt[, i] = as.numeric(unlist(out[1]))
}
 
#Calculate the kolmogorov smirnoff statisitc
#To calculate it we create data in such a way to create ecdf for binned data
#That means we will put each datapoint at the end of its bin
counter = 0
ecdf_data = rep(0, n)
for (i in 1:length(num_inbin)) {
  for(j in 1:num_inbin[i]) {
    if (num_inbin[i] == 0) {
      next
    }
    ecdf_data[j + counter] = data$endpoint[i]
  }
  counter = counter + num_inbin[i]
}
ecdf_binned = ecdf(ecdf_data) #now calculate ecdf using the prepared data
grid = seq(from = 0, to = 3.25, by = 0.01) #grid to compare distribution functions on (for KS statistic)
kol_smir = 0 
for (i in 1:length(grid)) {
  a = abs(ecdf_binned(grid[i]) - 
            cdfbilognorm(grid[i], mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt))
  if (a > kol_smir) {
    kol_smir = a
  }
}

#calculate T_b^* (similar to KS statistic)
T_b = rep(0, n_bootstrap)
for (k in 1:n_bootstrap) {
  counter = 0
  ecdf_boot_data = rep(0, ndata_boot)
  for (i in 1:length(data$X)) {
    for(j in 1:binned_bootstrap_data[i, k]) {
      if (binned_bootstrap_data[i, k] == 0) {
        next
      }
      ecdf_boot_data[j + counter] = data$endpoint[i] 
    }
    counter = counter + binned_bootstrap_data[i, k]
  }
  ecdf_boot = ecdf(ecdf_boot_data)
  for (j in 1:length(grid)) {
    a = abs(ecdf_boot(grid[j]) - 
              cdfbilognorm(grid[j], estim_boot_opt[1, k], estim_boot_opt[2, k], 
                           estim_boot_opt[3, k], estim_boot_opt[4, k], estim_boot_opt[5, k]))
    if (a > T_b[k]) {
      T_b[k] = a
    }
  }
}

#calculate the estimated p value
a = 0
for (k in 1:n_bootstrap) {
  if (T_b[k] > kol_smir) {
    a = a + 1
  }
}
p = 1 / (n_bootstrap + 1) * (a + 1)
print(p)
```
Performing these calculations we obvserve a p-value of 0.7722772. This is quite a large p-value and hence we do not reject $H_0$, which supports the assumption of a bi-log-normal model for our data. So we can use our optimized parameters to simulate new datapoints of our snowflake experiment, as described below Table (\@ref(tab:paramtable)). To finish this section, we provide a final plot containing the empirical CDF of our data and the parametrized CDF using our optimized parameters in Figure (\@ref(fig:CDF)).

```{r CDF, echo=FALSE, fig.align='center', fig.cap="Empirical CDF of the original binned data with a bi-log-normal CDF parametrized by our optimized parameters."}
ecdf_param = cdfbilognorm(grid, mu1_opt, mu2_opt, sigma1_opt, sigma2_opt, tau_opt)
ggplot() + 
  stat_ecdf(aes(x = ecdf_data, color = "Empirical CDF")) + 
  geom_line(aes(x = grid, y = ecdf_param, color = "Bi-Log-Normal CDF")) + 
  scale_color_manual(name = "",
                     breaks=c('Empirical CDF', 'Bi-Log-Normal CDF'),
                     values=c('Empirical CDF'='red', 'Bi-Log-Normal CDF'='blue')) + 
  labs(x = "x", y = "y", title = "Bi-Log-Normal CDF and Empirical CDF")
```
We see that a bi-log-normal distribution parametrized by our optimized parameters is a very good model for the binned data.

# Conclusion

In this report, we were able to find statistical arguments to show that the snowflake diameters follow a bi-log-normal distribution. In addition to that, using the EM algorithm and optimization of the binned log likelihood, we were able to find a set of parameter estimates such that our parametrized bi-log-normal model describes the data very well. These estimates can be found in Table (\@ref(tab:paramtable)), and below an instruction on how to simulate new snowflake diameters using these estimates.

The steps in this report show that even if we only have access to binned data there are possibilities to find an appropriate statistical model for the data. However, this requires some extra steps and we would always prefer non-binned data as it provides more information.