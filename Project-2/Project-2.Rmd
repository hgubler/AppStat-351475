---
title: "Project-1"
author: "Hannes Gubler"
date: "14.03.2023"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

# Introduction

In this report, we work with online shopping data where one observation corresponds to sessions, i.e instances of a user visiting an e-commerce website. These sessions take some time and may or may not lead to a purchase. We will fit a logistic regression model for the binary response variable "purchase", where we have 17 predictors available. 
The outline of the remainder of this project is as follows: We start by graphically exploring and wrangling the data. After that we use the prepared data to build a logistic regression. Finally we evaluate the performance of our final model using the Area Under Curve (AUC) criterion.

# Data Exploration and Wrangling
 
 As mentioned, we have 17 predictors available to build a model for the binary response "purchase". These predictor give information about the user's session. As a first step we change the names of the variables to have a consistent naming style. After these name changes, we have the following predictors.
 
* `administrative` Number of administrative-type pages that the user visited.
* `administrative_duration` Time spent on administrative pages.
* `informational` Number of informational-type pages visited.
* `informational_duration` Time spent on informational-type pages.
* `product_related` Number of product-related-type pages visited.
* `product_related_duration` Time spent on product-related-type pages.
* `bounce_rates` Average bounce rate of pages visited (for a specific webpage. the bounce rate is the percentage of users who enter and leave the webpage without triggering any request during their sessions).
* `exit_rates` Average exit rate of pages visited (for a specific webpage, the exit rate is the proportion of page views to the page that were last in the session).
* `page_values` Average page value of pages visited (for a specific webpage, the page value gives an idea of how much each page contributes to the site’ revenue).
* `special_day` Value in [0, 1] indicating closeness of the session to a special day (e.g. Mother’s day, etc.).
* `month` Which month the session took place.
* `operating_systems` Operating systems of the users coded as integers.
* `browser` Web browsers of the users coded as integers.
* `region` Geographic region in which the user is located coded as integers.
* `traffic_type` Where from the user arrived at the site (e.g. ad banner, SMS link, direct URL, etc.) coded as integers.
* `visitor_type` Self explanator, e.g. returning or new visitor to the webpage.
* `weekend` Binary indicator of whether the session took place during a weekend.

The next step is to decide which variables we include as factors in the model. For our model, we use "month", "operating_systems", "browser", "region", "traffic_type", "visitor_type" and "weekend" as factors, while the remaining predictors are treated as numeric variables. 
```{r datawrangle}
library(dplyr)
library(tidyr)
library(ggplot2)
library(pROC)
library(forcats)
library(ggcorrplot)
library(car)
load("2_online_shopping.RData")
data = Data # rename to "data"
rm(Data)
# first change the name to snake_case
data = data %>% rename(purchase = Revenue, administrative = Administrative,
                       administrative_duration = Administrative_Duration, 
                       informational = Informational, informational_duration = Informational_Duration,
                       product_related = ProductRelated, product_related_duration = ProductRelated_Duration,
                       bounce_rates = BounceRates, exit_rates = ExitRates,
                       page_values = PageValues, special_day = SpecialDay, month = Month,
                       operating_systems = OperatingSystems, browser = Browser, region = Region,
                       traffic_type = TrafficType, visitor_type = VisitorType, weekend = Weekend) 
#str(data) # to see which variables are factors and which are numeric
data = data %>% mutate_if(is.character, as.factor) # change character variables to factors
data = data %>% mutate_if(is.integer, as.numeric) # change integers variables to numeric
data = data %>% mutate_if(is.logical, as.factor) # change logical variables to factors
data = data %>% mutate_at(c("visitor_type", "region", "browser", "operating_systems",
                            "traffic_type"), as.factor) # individually change 
# some variables to factors
```

Now we can graphically explore our variables - by looking at the histograms.
```{r histograms, fig.align='center', fig.width=12, fig.height=8, fig.cap="Histograms of all the variables in our dataset."}
data_num = data %>% mutate_if(is.factor, as.numeric) # data with all variables as numeric
data_long <- gather(data_num, key = "predictor", value = "value")

# create histogram plot with facets
ggplot(data_long, aes(x = value)) + 
  geom_histogram() + 
  facet_wrap(~ predictor, scales = "free") +
  ggtitle("Histograms of our Data")
```

We see that some variables we treat as factors have some levels with very few observations (e.g the factors "operating systems" or "browser"). Levels with a small number of observations can be problematic because the model fits a single parameter for these levels which will have high variance due to the small number of datapoints. Following this reasoning we merge small levels (less than 30 observations) within one factor variable into a group called "others". 

For the numerical variables, we can see that a lot of them have a right scewed histogram, so we could log transform them to have more evenly spread out values. However they all have a lot of zero values so it we rather take the square root of those variables, as it has a similar effect as the logarithm. Namely we transform the numerical variables "administrative" "administrative_duration", "bounce_rates", "exit_rates", "Informational", "Informational_duration", "page_values", "product_related" and "product_related_duration" by applying the square root.

```{r factormerge&transformations}
# loop over the factor variables and merge categories with small number of 
# observations together
for (i in 1:length(data)) {
  if(is.factor(data[, i])) {
    tt = table(data[, i])
    data[, i] = fct_collapse(data[, i], "other" = names(tt[tt < 40]))
  }
}

# take the square root of the variables mentioned above
data = data %>% 
  mutate(administrative = administrative^(1/2),
         administrative_duration = administrative_duration^(1/2), 
         bounce_rates = bounce_rates^(1/2),
         exit_rates = exit_rates^(1/2),
         informational = informational^(1/2),
         informational_duration = informational_duration^(1/2), 
         page_values = page_values^(1/2), 
         product_related = product_related^(1/2),
         product_related_duration = product_related_duration^(1/2))
```


Now we are ready to build our first models in the next section.

# Logistic Regression Model Building

In this section we stepwise build a logistic regression model for our data. To compare two different models, we use deviance tests. 

TBD: Introduce deviance test

Question: Why use deviance test and not just likelihood ratio test with chi square?

For a starting point, we fit a logistic model including all the predictors available in the data that we prepared in the section before. After that we look at the Type II Anova table. Type II Anova compares, for each predictor, the full model with the model excluding that specific predictor by a deviance test.

```{r type2anova}
gm_all = glm(purchase ~ ., data=data, family="binomial") # model including all predictors
anova = Anova(gm_all, type = "II", test.statistic = "LR") # perform type 2 Anova on full
# model
library(knitr)
kable(anova, caption = "Type II Anova table for a logistic regression model including all predictors from the data.")
```

The type II Anova table suggests us that the predictors "product_related_duration", "bounce_rates", "exit_rates", "page_vales", "month", "traffic_type" and "visitor_type" are significant at level 0.05. So we proceed with a model containing only these predictors. For this model, we analyze the residuals by plotting the residuals vs each predictor and adding a smoother to the plots.

```{r residualplot, fig.align='center', fig.cap="Residuals vs predictor plot with a smoother. The residuals come from a logistic regression model using only the variables that were selected by the type II Anova table"}
# build a model including only the significant predictors from anova type 2 
gm_sig = glm(purchase ~ product_related_duration + 
               bounce_rates + 
               exit_rates + 
               page_values +
               month + 
               traffic_type + 
               visitor_type,
             data=data, family="binomial") 

# create residuals plot for each predictor in one figure 
data_num %>% 
  select(c(product_related_duration, bounce_rates, exit_rates, page_values, month,
           traffic_type, visitor_type)) %>%
  mutate(res = resid(gm_sig)) %>%
  pivot_longer(-res) %>% 
  ggplot(aes(y = res, x = value)) + 
  facet_wrap( ~ name, scales = "free") + 
  geom_point() + 
  geom_smooth() + # add a smoother to the residual plots
  ylab("Residuals") +
  xlab("Predictor Value") +
  ggtitle("Residuals vs Predictor")
```
 First note that the residuals are often organized in two clouds since the response variable is binary, so it is useful to use a smoother to detect patterns. Also, it looks like we could have a quadratic dependence on page_values as well as some non-linear dependence on product_related_duration.
 
 ## Testing for Interactions
 
 ## Final model
 
 TBD Give coefficients of final model
 
 ## Evaliate AUC
 
 maybe also evaluate accuracy?

