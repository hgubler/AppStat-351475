---
title: "rough_work"
author: "Hannes"
date: "2023-05-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
```

# Data Exploration

```{r}
library("tidyverse")
data_C02_atm = read.csv(file = "data/CO2_atmosphere.csv")
data_C02_emi = read.csv(file = "data/CO2_emissions.csv")
data_temp = read.csv(file = "data/temperature.csv")
data_temp = pivot_longer(data_temp, cols = c("Jan", "Feb", "Mar", 
                                             "Apr", "May", "Jun",
                                             "Jul", "Aug", "Sep", 
                                             "Oct", "Nov", "Dec"), 
                    names_to = "month", values_to = "temp")
temp = as.numeric(data_temp$temp)
ts_temp = ts(temp, start = 1880, frequency = 12)
plot(ts_temp)
```

We see a clear trend (change in mean). Seasoning not clearly visible, maybe because measurements are anomalies. Variance seems to be constant so no transformation.

Fit a linear model with linear dependence on time, and one with quadratic dependence on time and month as factor. Analyze its residuals

```{r}
lm_data = data.frame(y = temp, 
                     month = as.factor(rep(1:12,length(temp)/12)),
                     time = 1:length(temp))
lm_temp_lin = lm(y ~ ., data = lm_data)
lm_temp_quad = lm(y ~ . + I(time^2), data = lm_data)
lm_data$res_lin = resid(lm_temp_lin)
lm_data$res_quad = resid(lm_temp_quad)
lm_data$fit_lin = lm_temp_lin$fitted.values
lm_data$fit_quad = lm_temp_quad$fitted.values
fit_lin_plot = ggplot(lm_data, aes(x = time)) + 
  geom_line(aes(y = temp), linewidth = 0.5) +
  geom_line(aes(y = fit_lin, col = "red"), linewidth = 0.5)
fit_quad_plot = ggplot(lm_data, aes(x = time)) + 
  geom_line(aes(y = temp), linewidth = 0.5) +
  geom_line(aes(y = fit_quad, col = "red"), linewidth = 0.5)
res_lin_plot = ggplot(lm_data, aes(x = time, y = res_lin)) +
  geom_line()
res_quad_plot = ggplot(lm_data, aes(x = time, y = res_quad)) + 
  geom_line()
library(ggpubr)
ggarrange(fit_lin_plot, fit_quad_plot, res_lin_plot, res_quad_plot, nrow = 2, ncol = 2)
```

so quadratic trend seems reasonable? -> Hence twice differencing might be good? But if we look closely at the time series, it looks like piecewise linear trend. So could test both later.

Look at ACF and PACF

```{r}
library("DescTools")
PlotACF(ts_temp)
```

Exponential decay in ACF and cut off at lag 2 -> Chose AR2 based on this plot? (so 2,1,0) or (2,2,0) so far

Seasoning? Difference time series with lag 12 (because then we compare the measurements from this year with the last year, so the increase/decrease from this year to last year and hence we can analyze the dependencies of these lags to determine seasonal parameters), then look at ACF and PACF of it.

```{r}
diffed_ts_temp = diff(ts_temp, lag = 12)
par(mfrow = c(1, 2))
acf(diffed_ts_temp)
pacf(diffed_ts_temp)
```

-> Chose a c(2,0,1)_12 for seasonal parameters. The middle parameter is not completely clear as we are not sure if we observe seasonality or not. Intuitively there may be no seasonality since we are comparing anomalies. However it is still possible since most measurements were taken in the northern hemisphere and we could have a summer/winter difference. So we can test both (2,0,1)_12 or (2,1,1)_12 for the seasonal parameters

In general we were unsure about d = 1 or d = 2 and D = 0 or D = 1. Hence in total 4 models to test

Compare the models with AIC and 

```{r}
library(forecast)
library(knitr)
# fit potential models
ts_temp_fit_1_0 = Arima(ts_temp, order = c(2, 1, 0), seasonal = c(2, 0, 1))
ts_temp_fit_2_0 = Arima(ts_temp, order = c(2, 2, 0), seasonal = c(2, 0, 1))
ts_temp_fit_1_1 = Arima(ts_temp, order = c(2, 1, 0), seasonal = c(2, 1, 1))
ts_temp_fit_2_1 = Arima(ts_temp, order = c(2, 2, 0), seasonal = c(2, 1, 1))
# calculate AIC of models
AIC_1_0 = AIC(ts_temp_fit_1_0)
AIC_2_0 = AIC(ts_temp_fit_2_0)
AIC_1_1 = AIC(ts_temp_fit_1_1)
AIC_2_1 = AIC(ts_temp_fit_2_1)
AIC = c(AIC_1_0, AIC_2_0, AIC_1_1, AIC_2_1)
# calculate predictive checking error (1 step)
n = length(temp)
train = 1:(n-floor(n/3)) # training data (2 / 3 of real data)
err = array(0, c(4, floor(n/3)-1+1))
for(j in 0:(floor(n/3)-1)) {
  ts_temp_fit_1_0_train = Arima(ts(temp[train + j], start = 1880, frequency = 12), 
                                order = c(2, 1, 0), seasonal = c(2, 0, 1))
  ts_temp_fit_2_0_train = Arima(ts(temp[train + j], start = 1880, frequency = 12), 
                                order = c(2, 2, 0), seasonal = c(2, 0, 1))
  ts_temp_fit_1_1_train = Arima(ts(temp[train + j], start = 1880, frequency = 12), 
                                order = c(2, 1, 0), seasonal = c(2, 1, 1))
  ts_temp_fit_2_1_train = Arima(ts(temp[train + j], start = 1880, frequency = 12), 
                                order = c(2, 2, 0), seasonal = c(2, 1, 1))
  err[1,j+1] = (temp[1 + end(train)[1]+j] - 
                      predict(ts_temp_fit_1_0_train, n.ahead=1)$pred)^2
  err[2,j+1] = (temp[1 + end(train)[1]+j] - 
                      predict(ts_temp_fit_2_0_train, n.ahead=1)$pred)^2
  err[3,j+1] = (temp[1 + end(train)[1]+j] - 
                      predict(ts_temp_fit_1_1_train, n.ahead=1)$pred)^2
  err[4,j+1] = (temp[1 + end(train)[1]+j] - 
                      predict(ts_temp_fit_2_1_train, n.ahead=1)$pred)^2
  print(j)
}
fit_comp = as.data.frame(AIC)
kable(fit_comp)
```

```{r}
par(mfrow = c(1, 2))
acf(ts_temp_fit$residuals)
pacf(ts_temp_fit$residuals)
```


```{r}
auto.arima(ts_temp)
```


Questions

* Why does lm fits not capture same amount of variance? 